{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJSKmjWtdChe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "from typing import List\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def regular_softmax(vec: List[float] ):\n",
        "    \"\"\"\n",
        "        y_i = e_^{x_i} / \\sum_{j=1}^{V} e_^{x_j}\n",
        "    \"\"\"\n",
        "    e = float(np.exp(1))\n",
        "    y = []\n",
        "    sum = 0\n",
        "    for val in vec:\n",
        "        temp = e**val\n",
        "        y.append(temp)\n",
        "        sum += temp\n",
        "    y = [i/sum for i in y]\n",
        "    return y\n",
        "\n",
        "vec = [1,2,3]\n",
        "print(regular_softmax(vec))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TF9vsnOdU87",
        "outputId": "39cd6b68-6f84-439f-f4bc-19d22af02087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09003057317038046, 0.24472847105479764, 0.6652409557748218]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor(vec).float(), dim=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrfx6ZjqiQHY",
        "outputId": "ff41bed2-3d7e-4166-cb3f-c56799167d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0900, 0.2447, 0.6652])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scipy.special.softmax(vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzPwBVNYiXFi",
        "outputId": "9ac723ae-c3fe-48d2-c36c-afeed50e3217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09003057, 0.24472847, 0.66524096])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_softmax(vec: List[float] ):\n",
        "    \"\"\"\n",
        "        y_i = e_^{x_i - max} / \\sum_{j=1}^{V} e_^{x_j - max}\n",
        "    \"\"\"\n",
        "    y = []\n",
        "    sum = 0\n",
        "    max_val = max(vec)\n",
        "    for val in vec:\n",
        "        temp = np.exp(val - max_val)\n",
        "        y.append(temp)\n",
        "        sum += temp\n",
        "    y = y/sum\n",
        "    return y\n",
        "\n",
        "vec = [1,2,3]\n",
        "print(safe_softmax(vec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dze7ejGVipuM",
        "outputId": "05cb8a3d-935d-4614-a175-b0f1006f8036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09003057 0.24472847 0.66524096]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_with_online_normilizer(vec: List[float] ):\n",
        "    y = []\n",
        "    e = float(np.exp(1))\n",
        "    m = -np.inf\n",
        "    d = 0\n",
        "    for val in vec:\n",
        "        m_prev = m\n",
        "        m = max(m, val)\n",
        "        d = d * e**(m_prev - m) + e**(val - m)\n",
        "    for val in vec:\n",
        "        y.append(e**(val - m) / d)\n",
        "    return y\n",
        "\n",
        "vec = [1,2,3]\n",
        "print(softmax_with_online_normilizer(vec))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_4j1epVjg2o",
        "outputId": "18fa1e42-e2f2-46b9-98e1-f6e7f444ccee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.09003057317038046, 0.24472847105479764, 0.6652409557748218]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NVIDIA/online-softmax"
      ],
      "metadata": {
        "id": "UNFnem46qBVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Math\n",
        "latex = r\"\"\"\n",
        "y_i = \\frac{e^{x_i - m}}{\\sum_{j=1}^{N} e^{x_i - m} }\n",
        "\\\\\n",
        "d_V\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "Math(latex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "yIup5478jixq",
        "outputId": "3d3df18a-f134-4b9f-e5de-8ce0ea96f6c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Math object>"
            ],
            "text/latex": "$\\displaystyle \ny_i = \\frac{e^{x_i - m}}{\\sum_{j=1}^{N} e^{x_i - m} }\n\\\\\nd_V\n\n$"
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /online_softmax_benchmark.cu\n",
        "\n",
        "#include <algorithm>\n",
        "#include <cassert>\n",
        "#include <cfloat>\n",
        "#include <cub/cub.cuh>\n",
        "#include <curand.h>\n",
        "#include <iomanip>\n",
        "#include <iostream>\n",
        "#include <limits>\n",
        "#include <math.h>\n",
        "#include <stdio.h>\n",
        "#include <string>\n",
        "#include <tuple>\n",
        "#include <vector>\n",
        "\n",
        "#define CUDA_CHECK(callstr) {cudaError_t error_code = callstr; if (error_code != cudaSuccess) { std::cerr << \"CUDA error \" << error_code << \" at \" << __FILE__ << \":\" << __LINE__; assert(0); } }\n",
        "#define CURAND_CHECK(callstr) {curandStatus_t error_code = callstr; if (error_code != CURAND_STATUS_SUCCESS) { std::cerr << \"cuRAND error \" << error_code << \" at \" << __FILE__ << \":\" << __LINE__; assert(0); } }\n",
        "\n",
        "const int MAX_K=5;\n",
        "\n",
        "enum SOFTMAX_TYPE\n",
        "{\n",
        "    SOFTMAX_TYPE_NAIVE,\n",
        "    SOFTMAX_TYPE_SAFE,\n",
        "    SOFTMAX_TYPE_ONLINE\n",
        "};\n",
        "\n",
        "enum SOFTMAX_TOPK_TYPE\n",
        "{\n",
        "    SOFTMAX_TOPK_TYPE_TOPK_ONLY,\n",
        "    SOFTMAX_TOPK_TYPE_SAFE_UNFUSED,\n",
        "    SOFTMAX_TOPK_TYPE_SAFE_FUSED,\n",
        "    SOFTMAX_TOPK_TYPE_ONLINE_FUSED\n",
        "};\n",
        "\n",
        "std::string getSoftmaxTypeName(SOFTMAX_TYPE t)\n",
        "{\n",
        "    switch (t)\n",
        "    {\n",
        "    case SOFTMAX_TYPE_NAIVE:\n",
        "        return \"Naive Softmax\";\n",
        "    case SOFTMAX_TYPE_SAFE:\n",
        "        return \"Safe Softmax\";\n",
        "    case SOFTMAX_TYPE_ONLINE:\n",
        "        return \"Online Softmax\";\n",
        "    default:\n",
        "        assert(0);\n",
        "        break;\n",
        "    }\n",
        "    return \"\";\n",
        "}\n",
        "\n",
        "std::string getSoftmaxTopkTypeName(SOFTMAX_TOPK_TYPE t)\n",
        "{\n",
        "    switch (t)\n",
        "    {\n",
        "    case SOFTMAX_TOPK_TYPE_TOPK_ONLY:\n",
        "        return \"TopK\";\n",
        "    case SOFTMAX_TOPK_TYPE_SAFE_UNFUSED:\n",
        "        return \"Safe Softmax + TopK unfused\";\n",
        "    case SOFTMAX_TOPK_TYPE_SAFE_FUSED:\n",
        "        return \"Safe Softmax + TopK fused\";\n",
        "    case SOFTMAX_TOPK_TYPE_ONLINE_FUSED:\n",
        "        return \"Online Softmax + TopK fused\";\n",
        "    default:\n",
        "        assert(0);\n",
        "        break;\n",
        "    }\n",
        "    return \"\";\n",
        "}\n",
        "\n",
        "template<int THREADBLOCK_SIZE>\n",
        "__launch_bounds__(THREADBLOCK_SIZE)\n",
        "__global__ void naive_softmax(\n",
        "    const float * __restrict x,\n",
        "    float * __restrict y,\n",
        "    int V)\n",
        "{\n",
        "    int thread_id = threadIdx.x;\n",
        "    int vector_id = blockIdx.x;\n",
        "\n",
        "    // reposition x and y to data for the current vector\n",
        "    x += vector_id * V;\n",
        "    y += vector_id * V;\n",
        "\n",
        "    typedef cub::BlockReduce<float, THREADBLOCK_SIZE> BlockReduce;\n",
        "\n",
        "    __shared__ typename BlockReduce::TempStorage temp_storage;\n",
        "    __shared__ float d_total_inverse;\n",
        "\n",
        "    float d_partial = 0.0F;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        d_partial += __expf(x[elem_id]);\n",
        "\n",
        "    float d = BlockReduce(temp_storage).Sum(d_partial);\n",
        "    if (thread_id == 0)\n",
        "        d_total_inverse = __fdividef(1.0F, d);\n",
        "    __syncthreads();\n",
        "\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        y[elem_id] = __expf(x[elem_id]) * d_total_inverse;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ float max_op(float a, float b)\n",
        "{\n",
        "    return fmaxf(a, b);\n",
        "}\n",
        "\n",
        "template<int THREADBLOCK_SIZE>\n",
        "__launch_bounds__(THREADBLOCK_SIZE)\n",
        "__global__ void safe_softmax(\n",
        "    const float * __restrict x,\n",
        "    float * __restrict y,\n",
        "    int V)\n",
        "{\n",
        "    int thread_id = threadIdx.x;\n",
        "    int vector_id = blockIdx.x;\n",
        "\n",
        "    // reposition x and y to data for the current vector\n",
        "    x += vector_id * V;\n",
        "    y += vector_id * V;\n",
        "\n",
        "    typedef cub::BlockReduce<float, THREADBLOCK_SIZE> BlockReduce;\n",
        "\n",
        "    __shared__ typename BlockReduce::TempStorage temp_storage;\n",
        "    __shared__ float m_total;\n",
        "    __shared__ float d_total_inverse;\n",
        "\n",
        "    float m_partial = -FLT_MAX;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        m_partial = max_op(m_partial, x[elem_id]);\n",
        "\n",
        "    float m = BlockReduce(temp_storage).Reduce(m_partial, max_op);\n",
        "    if (thread_id == 0)\n",
        "        m_total = m;\n",
        "    __syncthreads();\n",
        "\n",
        "    float d_partial = 0.0F;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        d_partial += __expf(x[elem_id] - m_total);\n",
        "\n",
        "    float d = BlockReduce(temp_storage).Sum(d_partial);\n",
        "    if (thread_id == 0)\n",
        "        d_total_inverse = __fdividef(1.0F, d);\n",
        "    __syncthreads();\n",
        "\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        y[elem_id] = __expf(x[elem_id] - m_total) * d_total_inverse;\n",
        "}\n",
        "\n",
        "struct __align__(8) MD\n",
        "{\n",
        "    float m;\n",
        "    float d;\n",
        "};\n",
        "\n",
        "__device__ __forceinline__ MD reduce_md_op(MD a, MD b)\n",
        "{\n",
        "    bool a_bigger = (a.m > b.m);\n",
        "    MD bigger_m = a_bigger ? a : b;\n",
        "    MD smaller_m = a_bigger ? b : a;\n",
        "    MD res;\n",
        "    res.d = bigger_m.d + smaller_m.d * __expf(smaller_m.m - bigger_m.m);\n",
        "    res.m = bigger_m.m;\n",
        "    return res;\n",
        "}\n",
        "\n",
        "template<int THREADBLOCK_SIZE>\n",
        "__launch_bounds__(THREADBLOCK_SIZE)\n",
        "__global__ void online_softmax(\n",
        "    const float * __restrict x,\n",
        "    float * __restrict y,\n",
        "    int V)\n",
        "{\n",
        "    int thread_id = threadIdx.x;\n",
        "    int vector_id = blockIdx.x;\n",
        "\n",
        "    // reposition x and y to data for the current vector\n",
        "    x += vector_id * V;\n",
        "    y += vector_id * V;\n",
        "\n",
        "    typedef cub::BlockReduce<MD, THREADBLOCK_SIZE> BlockReduce;\n",
        "\n",
        "    __shared__ typename BlockReduce::TempStorage temp_storage;\n",
        "    __shared__ MD md_total;\n",
        "\n",
        "    MD md_partial;\n",
        "    md_partial.m = -FLT_MAX;\n",
        "    md_partial.d = 0.0F;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "    {\n",
        "        MD new_elem;\n",
        "        new_elem.m = x[elem_id];\n",
        "        new_elem.d = 1.0F;\n",
        "        md_partial = reduce_md_op(md_partial, new_elem);\n",
        "    }\n",
        "\n",
        "    MD md = BlockReduce(temp_storage).Reduce(md_partial, reduce_md_op);\n",
        "    if (thread_id == 0)\n",
        "        md_total = md;\n",
        "    __syncthreads();\n",
        "\n",
        "    float d_total_inverse = __fdividef(1.0F, md_total.d);\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        y[elem_id] = __expf(x[elem_id] - md_total.m) * d_total_inverse;\n",
        "}\n",
        "\n",
        "template<int MAX_K>\n",
        "struct TopK\n",
        "{\n",
        "    int p[MAX_K];\n",
        "    float u[MAX_K];\n",
        "\n",
        "    __device__ __forceinline__ void insert(float elem, int elem_id)\n",
        "    {\n",
        "        if (elem > u[MAX_K-1])\n",
        "        {\n",
        "            u[MAX_K-1] = elem;\n",
        "            p[MAX_K-1] = elem_id;\n",
        "        }\n",
        "        for(int k = MAX_K - 2; k >= 0; --k)\n",
        "        {\n",
        "            if (u[k+1] > u[k])\n",
        "            {\n",
        "                float u2 = u[k];\n",
        "                int p2 = p[k];\n",
        "                u[k] = u[k+1];\n",
        "                p[k] = p[k+1];\n",
        "                u[k+1] = u2;\n",
        "                p[k+1] = p2;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "};\n",
        "\n",
        "template<int MAX_K>\n",
        "__device__ __forceinline__ TopK<MAX_K> reduce_topk_op(const TopK<MAX_K>& a, const TopK<MAX_K>& b)\n",
        "{\n",
        "    TopK<MAX_K> res = a;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        res.insert(b.u[i], b.p[i]);\n",
        "    return res;\n",
        "}\n",
        "\n",
        "template<int MAX_K, int THREADBLOCK_SIZE>\n",
        "__launch_bounds__(THREADBLOCK_SIZE)\n",
        "__global__ void topk(\n",
        "    const float * __restrict y,\n",
        "    int * __restrict z,\n",
        "    float * __restrict v,\n",
        "    int V,\n",
        "    int K)\n",
        "{\n",
        "    int thread_id = threadIdx.x;\n",
        "    int vector_id = blockIdx.x;\n",
        "\n",
        "    // reposition y to data for the current vector\n",
        "    y += vector_id * V;\n",
        "\n",
        "    typedef cub::BlockReduce<TopK<MAX_K>, THREADBLOCK_SIZE> BlockReduce;\n",
        "\n",
        "    __shared__ typename BlockReduce::TempStorage temp_storage;\n",
        "\n",
        "    TopK<MAX_K> partial;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        partial.p[i] = -1;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        partial.u[i] = -FLT_MAX;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "    {\n",
        "        float elem = y[elem_id];\n",
        "        partial.insert(elem, elem_id);\n",
        "    }\n",
        "\n",
        "    TopK<MAX_K> total = BlockReduce(temp_storage).Reduce(partial, reduce_topk_op<MAX_K>);\n",
        "\n",
        "    if (thread_id == 0)\n",
        "    {\n",
        "        z += vector_id * K;\n",
        "        v += vector_id * K;\n",
        "\n",
        "        for(int i = 0; i < MAX_K; ++i)\n",
        "        {\n",
        "            if (i < K)\n",
        "            {\n",
        "                z[i] = total.p[i];\n",
        "                v[i] = total.u[i];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "template<int MAX_K>\n",
        "struct TopKD\n",
        "{\n",
        "    float d;\n",
        "    TopK<MAX_K> topk;\n",
        "};\n",
        "\n",
        "template<int MAX_K>\n",
        "__device__ __forceinline__ TopKD<MAX_K> reduce_topk_d_op(const TopKD<MAX_K>& a, const TopKD<MAX_K>& b)\n",
        "{\n",
        "    TopKD<MAX_K> res;\n",
        "    res.d = a.d + b.d;\n",
        "    res.topk = reduce_topk_op(a.topk, b.topk);\n",
        "    return res;\n",
        "}\n",
        "\n",
        "template<int MAX_K, int THREADBLOCK_SIZE>\n",
        "__launch_bounds__(THREADBLOCK_SIZE)\n",
        "__global__ void safe_softmax_topk(\n",
        "    const float * __restrict x,\n",
        "    int * __restrict z,\n",
        "    float * __restrict v,\n",
        "    int V,\n",
        "    int K)\n",
        "{\n",
        "    int thread_id = threadIdx.x;\n",
        "    int vector_id = blockIdx.x;\n",
        "\n",
        "    // reposition y to data for the current vector\n",
        "    x += vector_id * V;\n",
        "\n",
        "    typedef cub::BlockReduce<float, THREADBLOCK_SIZE> MaxValBlockReduce;\n",
        "    typedef cub::BlockReduce<TopKD<MAX_K>, THREADBLOCK_SIZE> BlockReduce;\n",
        "\n",
        "    __shared__ typename MaxValBlockReduce::TempStorage max_val_temp_storage;\n",
        "    __shared__ typename BlockReduce::TempStorage temp_storage;\n",
        "    __shared__ float m_total;\n",
        "\n",
        "    float m_partial = -FLT_MAX;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "        m_partial = max_op(m_partial, x[elem_id]);\n",
        "\n",
        "    float m = MaxValBlockReduce(max_val_temp_storage).Reduce(m_partial, max_op);\n",
        "    if (thread_id == 0)\n",
        "        m_total = m;\n",
        "    __syncthreads();\n",
        "\n",
        "    TopKD<MAX_K> partial;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        partial.topk.p[i] = -1;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        partial.topk.u[i] = -FLT_MAX;\n",
        "    partial.d = 0.0F;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "    {\n",
        "        float elem = x[elem_id];\n",
        "        partial.d += __expf(elem - m_total);\n",
        "        partial.topk.insert(elem, elem_id);\n",
        "    }\n",
        "\n",
        "    TopKD<MAX_K> total = BlockReduce(temp_storage).Reduce(partial, reduce_topk_d_op<MAX_K>);\n",
        "\n",
        "    if (thread_id == 0)\n",
        "    {\n",
        "        z += vector_id * K;\n",
        "        v += vector_id * K;\n",
        "\n",
        "        float d_total_inverse = __fdividef(1.0F, total.d);\n",
        "        for(int i = 0; i < MAX_K; ++i)\n",
        "        {\n",
        "            float val = __expf(total.topk.u[i] - m_total) * d_total_inverse;\n",
        "            if (i < K)\n",
        "            {\n",
        "                z[i] = total.topk.p[i];\n",
        "                v[i] = val;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "template<int MAX_K>\n",
        "struct TopKMD\n",
        "{\n",
        "    MD md;\n",
        "    TopK<MAX_K> topk;\n",
        "};\n",
        "\n",
        "template<int MAX_K>\n",
        "__device__ __forceinline__ TopKMD<MAX_K> reduce_topk_md_op(const TopKMD<MAX_K>& a, const TopKMD<MAX_K>& b)\n",
        "{\n",
        "    TopKMD<MAX_K> res;\n",
        "    res.md = reduce_md_op(a.md, b.md);\n",
        "    res.topk = reduce_topk_op(a.topk, b.topk);\n",
        "    return res;\n",
        "}\n",
        "\n",
        "template<int MAX_K, int THREADBLOCK_SIZE>\n",
        "__launch_bounds__(THREADBLOCK_SIZE)\n",
        "__global__ void online_softmax_topk(\n",
        "    const float * __restrict x,\n",
        "    int * __restrict z,\n",
        "    float * __restrict v,\n",
        "    int V,\n",
        "    int K)\n",
        "{\n",
        "    int thread_id = threadIdx.x;\n",
        "    int vector_id = blockIdx.x;\n",
        "\n",
        "    // reposition y to data for the current vector\n",
        "    x += vector_id * V;\n",
        "\n",
        "    typedef cub::BlockReduce<TopKMD<MAX_K>, THREADBLOCK_SIZE> BlockReduce;\n",
        "\n",
        "    __shared__ typename BlockReduce::TempStorage temp_storage;\n",
        "\n",
        "    TopKMD<MAX_K> partial;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        partial.topk.p[i] = -1;\n",
        "    for(int i = 0; i < MAX_K; ++i)\n",
        "        partial.topk.u[i] = -FLT_MAX;\n",
        "    partial.md.m = -FLT_MAX;\n",
        "    partial.md.d = 0.0F;\n",
        "    for(int elem_id = thread_id; elem_id < V; elem_id += THREADBLOCK_SIZE)\n",
        "    {\n",
        "        float elem = x[elem_id];\n",
        "        MD new_elem{elem, 1.0F};\n",
        "        partial.md = reduce_md_op(partial.md, new_elem);\n",
        "        partial.topk.insert(elem, elem_id);\n",
        "    }\n",
        "\n",
        "    TopKMD<MAX_K> total = BlockReduce(temp_storage).Reduce(partial, reduce_topk_md_op<MAX_K>);\n",
        "\n",
        "    if (thread_id == 0)\n",
        "    {\n",
        "        z += vector_id * K;\n",
        "        v += vector_id * K;\n",
        "\n",
        "        float d_total_inverse = __fdividef(1.0F, total.md.d);\n",
        "        for(int i = 0; i < MAX_K; ++i)\n",
        "        {\n",
        "            float val = __expf(total.topk.u[i] - total.md.m) * d_total_inverse;\n",
        "            if (i < K)\n",
        "            {\n",
        "                z[i] = total.topk.p[i];\n",
        "                v[i] = val;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void fill_random_values(float * x, int count)\n",
        "{\n",
        "    curandGenerator_t gen;\n",
        "    CURAND_CHECK(curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT));\n",
        "    CURAND_CHECK(curandSetPseudoRandomGeneratorSeed(gen, 1234ULL));\n",
        "    CURAND_CHECK(curandGenerateUniform(gen, x, count));\n",
        "    CURAND_CHECK(curandDestroyGenerator(gen));\n",
        "}\n",
        "\n",
        "std::vector<float> run_softmax(int V, int batch_size, SOFTMAX_TYPE t)\n",
        "{\n",
        "    float * x;\n",
        "    float * y;\n",
        "    CUDA_CHECK(cudaMalloc(&x, (size_t)V * batch_size * sizeof(float)));\n",
        "    fill_random_values(x, V * batch_size);\n",
        "    CUDA_CHECK(cudaMalloc(&y, (size_t)V * batch_size * sizeof(float)));\n",
        "\n",
        "    switch (t)\n",
        "    {\n",
        "    case SOFTMAX_TYPE_NAIVE:\n",
        "        naive_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "        break;\n",
        "    case SOFTMAX_TYPE_SAFE:\n",
        "        safe_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "        break;\n",
        "    case SOFTMAX_TYPE_ONLINE:\n",
        "        online_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "        break;\n",
        "    default:\n",
        "        assert(0);\n",
        "    }\n",
        "\n",
        "    std::vector<float> res(V * batch_size);\n",
        "    CUDA_CHECK(cudaMemcpy(&res[0], y, V * batch_size * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    return res;\n",
        "}\n",
        "\n",
        "void compare_softmax_results(int V, int batch_size, SOFTMAX_TYPE t1, SOFTMAX_TYPE t2)\n",
        "{\n",
        "    std::vector<float> res1 = run_softmax(V, batch_size, t1);\n",
        "    std::vector<float> res2 = run_softmax(V, batch_size, t2);\n",
        "\n",
        "    float max_diff = 0.0F;\n",
        "    double total_diff = 0.0F;\n",
        "    for(int i = 0; i < res1.size(); ++i)\n",
        "    {\n",
        "        float diff = fabs(res1[i] - res2[i]);\n",
        "        max_diff = std::max(max_diff, diff);\n",
        "        total_diff += diff;\n",
        "    }\n",
        "    std::cout << \"Comparing \" << getSoftmaxTypeName(t1) << \" and \" << getSoftmaxTypeName(t2)\n",
        "        << \": Max diff = \" << max_diff << \", Avg diff = \" << (float)(total_diff / res1.size()) << std::endl;\n",
        "}\n",
        "\n",
        "// Returns runtime, in seconds\n",
        "float benchmark_softmax(int V, int batch_size, SOFTMAX_TYPE t, int run_iterations)\n",
        "{\n",
        "    float * x;\n",
        "    float * y;\n",
        "    CUDA_CHECK(cudaMalloc(&x, (size_t)V * batch_size * sizeof(float)));\n",
        "    fill_random_values(x, V * batch_size);\n",
        "    CUDA_CHECK(cudaMalloc(&y, (size_t)V * batch_size * sizeof(float)));\n",
        "\n",
        "    // Heuristic to have at least 8 iterations of the loop\n",
        "    int max_threadblock_size = V / 8;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start, 0));\n",
        "    for(int i = 0; i < run_iterations; ++i)\n",
        "    {\n",
        "        switch (t)\n",
        "        {\n",
        "        case SOFTMAX_TYPE_NAIVE:\n",
        "            if (max_threadblock_size >= 256)\n",
        "                naive_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "            else if (max_threadblock_size >= 128)\n",
        "                naive_softmax<128><<<batch_size,128>>>(x, y, V);\n",
        "            else if (max_threadblock_size >= 64)\n",
        "                naive_softmax<64><<<batch_size,64>>>(x, y, V);\n",
        "            else\n",
        "                naive_softmax<32><<<batch_size,32>>>(x, y, V);\n",
        "            break;\n",
        "        case SOFTMAX_TYPE_SAFE:\n",
        "            if (max_threadblock_size >= 256)\n",
        "                safe_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "            else if (max_threadblock_size >= 128)\n",
        "                safe_softmax<128><<<batch_size,128>>>(x, y, V);\n",
        "            else if (max_threadblock_size >= 64)\n",
        "                safe_softmax<64><<<batch_size,64>>>(x, y, V);\n",
        "            else\n",
        "                safe_softmax<32><<<batch_size,32>>>(x, y, V);\n",
        "            break;\n",
        "        case SOFTMAX_TYPE_ONLINE:\n",
        "            if (max_threadblock_size >= 256)\n",
        "                online_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "            else if (max_threadblock_size >= 128)\n",
        "                online_softmax<128><<<batch_size,128>>>(x, y, V);\n",
        "            else if (max_threadblock_size >= 64)\n",
        "                online_softmax<64><<<batch_size,64>>>(x, y, V);\n",
        "            else\n",
        "                online_softmax<32><<<batch_size,32>>>(x, y, V);\n",
        "            break;\n",
        "        default:\n",
        "            assert(0);\n",
        "        }\n",
        "        CUDA_CHECK(cudaGetLastError());\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop, 0));\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "    float elapsedTime;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&elapsedTime, start, stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    CUDA_CHECK(cudaFree(x));\n",
        "    CUDA_CHECK(cudaFree(y));\n",
        "\n",
        "    return elapsedTime / run_iterations * 0.001F;\n",
        "}\n",
        "\n",
        "// Returns runtime, in seconds\n",
        "float benchmark_softmax_topk(int V, int K, int batch_size, SOFTMAX_TOPK_TYPE t, int run_iterations)\n",
        "{\n",
        "    assert(K<=MAX_K);\n",
        "\n",
        "    float * x;\n",
        "    float * y;\n",
        "    int * z;\n",
        "    float * v;\n",
        "    CUDA_CHECK(cudaMalloc(&x, (size_t)V * batch_size * sizeof(float)));\n",
        "    fill_random_values(x, V * batch_size);\n",
        "    CUDA_CHECK(cudaMalloc(&y, (size_t)V * batch_size * sizeof(float)));\n",
        "    fill_random_values(y, V * batch_size);\n",
        "    CUDA_CHECK(cudaMalloc(&z, (size_t)K * batch_size * sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&v, (size_t)K * batch_size * sizeof(float)));\n",
        "\n",
        "    // Heuristic to have at least 16 iterations of the loop\n",
        "    int max_threadblock_size = V / 16;\n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    CUDA_CHECK(cudaEventCreate(&start));\n",
        "    CUDA_CHECK(cudaEventCreate(&stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventRecord(start, 0));\n",
        "    for(int i = 0; i < run_iterations; ++i)\n",
        "    {\n",
        "        switch (t)\n",
        "        {\n",
        "        case SOFTMAX_TOPK_TYPE_TOPK_ONLY:\n",
        "            if (max_threadblock_size >= 256)\n",
        "                topk<MAX_K,256><<<batch_size,256>>>(y, z, v, V, K);\n",
        "            else if (max_threadblock_size >= 128)\n",
        "                topk<MAX_K,128><<<batch_size,128>>>(y, z, v, V, K);\n",
        "            else if (max_threadblock_size >= 64)\n",
        "                topk<MAX_K,64><<<batch_size,64>>>(y, z, v, V, K);\n",
        "            else\n",
        "                topk<MAX_K,32><<<batch_size,32>>>(y, z, v, V, K);\n",
        "            break;\n",
        "        case SOFTMAX_TOPK_TYPE_SAFE_UNFUSED:\n",
        "            if (max_threadblock_size >= 256)\n",
        "            {\n",
        "                safe_softmax<256><<<batch_size,256>>>(x, y, V);\n",
        "                topk<MAX_K,256><<<batch_size,256>>>(y, z, v, V, K);\n",
        "            }\n",
        "            else if (max_threadblock_size >= 128)\n",
        "            {\n",
        "                safe_softmax<128><<<batch_size,128>>>(x, y, V);\n",
        "                topk<MAX_K,128><<<batch_size,128>>>(y, z, v, V, K);\n",
        "            }\n",
        "            else if (max_threadblock_size >= 64)\n",
        "            {\n",
        "                safe_softmax<64><<<batch_size,64>>>(x, y, V);\n",
        "                topk<MAX_K,64><<<batch_size,64>>>(y, z, v, V, K);\n",
        "            }\n",
        "            else\n",
        "            {\n",
        "                safe_softmax<32><<<batch_size,32>>>(x, y, V);\n",
        "                topk<MAX_K,32><<<batch_size,32>>>(y, z, v, V, K);\n",
        "            }\n",
        "            break;\n",
        "        case SOFTMAX_TOPK_TYPE_SAFE_FUSED:\n",
        "            if (max_threadblock_size >= 256)\n",
        "                safe_softmax_topk<MAX_K,256><<<batch_size,256>>>(x, z, v, V, K);\n",
        "            else if (max_threadblock_size >= 128)\n",
        "                safe_softmax_topk<MAX_K,128><<<batch_size,128>>>(x, z, v, V, K);\n",
        "            else if (max_threadblock_size >= 64)\n",
        "                safe_softmax_topk<MAX_K,64><<<batch_size,64>>>(x, z, v, V, K);\n",
        "            else\n",
        "                safe_softmax_topk<MAX_K,32><<<batch_size,32>>>(x, z, v, V, K);\n",
        "            break;\n",
        "        case SOFTMAX_TOPK_TYPE_ONLINE_FUSED:\n",
        "            if (max_threadblock_size >= 256)\n",
        "                online_softmax_topk<MAX_K,256><<<batch_size,256>>>(x, z, v, V, K);\n",
        "            else if (max_threadblock_size >= 128)\n",
        "                online_softmax_topk<MAX_K,128><<<batch_size,128>>>(x, z, v, V, K);\n",
        "            else if (max_threadblock_size >= 64)\n",
        "                online_softmax_topk<MAX_K,64><<<batch_size,64>>>(x, z, v, V, K);\n",
        "            else\n",
        "                online_softmax_topk<MAX_K,32><<<batch_size,32>>>(x, z, v, V, K);\n",
        "            break;\n",
        "        default:\n",
        "            assert(0);\n",
        "        }\n",
        "        CUDA_CHECK(cudaGetLastError());\n",
        "    }\n",
        "    CUDA_CHECK(cudaEventRecord(stop, 0));\n",
        "    CUDA_CHECK(cudaEventSynchronize(stop));\n",
        "    float elapsedTime;\n",
        "    CUDA_CHECK(cudaEventElapsedTime(&elapsedTime, start, stop));\n",
        "\n",
        "    CUDA_CHECK(cudaEventDestroy(start));\n",
        "    CUDA_CHECK(cudaEventDestroy(stop));\n",
        "\n",
        "    CUDA_CHECK(cudaFree(x));\n",
        "    CUDA_CHECK(cudaFree(y));\n",
        "    CUDA_CHECK(cudaFree(z));\n",
        "    CUDA_CHECK(cudaFree(v));\n",
        "\n",
        "    return elapsedTime / run_iterations * 0.001F;\n",
        "}\n",
        "\n",
        "std::tuple<std::vector<float>,std::vector<int>,std::vector<float>> run_topk(int V, int K, int batch_size, SOFTMAX_TOPK_TYPE t)\n",
        "{\n",
        "    assert(K<=MAX_K);\n",
        "\n",
        "    float * y;\n",
        "    int * z;\n",
        "    float * v;\n",
        "    CUDA_CHECK(cudaMalloc(&y, (size_t)V * batch_size * sizeof(float)));\n",
        "    fill_random_values(y, V * batch_size);\n",
        "    CUDA_CHECK(cudaMalloc(&z, (size_t)K * batch_size * sizeof(int)));\n",
        "    CUDA_CHECK(cudaMalloc(&v, (size_t)K * batch_size * sizeof(float)));\n",
        "\n",
        "    switch (t)\n",
        "    {\n",
        "    case SOFTMAX_TOPK_TYPE_TOPK_ONLY:\n",
        "        topk<MAX_K,256><<<batch_size,256>>>(y, z, v, V, K);\n",
        "        break;\n",
        "    case SOFTMAX_TOPK_TYPE_SAFE_FUSED:\n",
        "        safe_softmax_topk<MAX_K,256><<<batch_size,256>>>(y, z, v, V, K);\n",
        "        break;\n",
        "    case SOFTMAX_TOPK_TYPE_ONLINE_FUSED:\n",
        "        online_softmax_topk<MAX_K,256><<<batch_size,256>>>(y, z, v, V, K);\n",
        "        break;\n",
        "    default:\n",
        "        assert(0);\n",
        "    }\n",
        "\n",
        "    std::vector<float> yh(V * batch_size);\n",
        "    std::vector<int> zh(K * batch_size);\n",
        "    std::vector<float> vh(K * batch_size);\n",
        "\n",
        "    CUDA_CHECK(cudaMemcpy(&yh[0], y, (size_t)V * batch_size * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(&zh[0], z, (size_t)K * batch_size * sizeof(int), cudaMemcpyDeviceToHost));\n",
        "    CUDA_CHECK(cudaMemcpy(&vh[0], v, (size_t)K * batch_size * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "    CUDA_CHECK(cudaFree(y));\n",
        "    CUDA_CHECK(cudaFree(z));\n",
        "    CUDA_CHECK(cudaFree(v));\n",
        "\n",
        "    return std::make_tuple(yh, zh, vh);\n",
        "}\n",
        "\n",
        "void compare_topk_results(int V, int K, int batch_size, SOFTMAX_TOPK_TYPE t)\n",
        "{\n",
        "    std::vector<float> yh;\n",
        "    std::vector<int> zh;\n",
        "    std::vector<float> vh;\n",
        "\n",
        "    std::tie(yh, zh, vh) = run_topk(V, K, batch_size, t);\n",
        "\n",
        "    auto y = yh.begin();\n",
        "    auto z = zh.begin();\n",
        "    auto v = vh.begin();\n",
        "    int mismatches = 0;\n",
        "    for(int i = 0; i < batch_size; ++i, y += V, z += K, v += K)\n",
        "    {\n",
        "        std::vector<std::pair<float,int>> elemsWithIndices;\n",
        "        for(int j = 0; j < V; ++j)\n",
        "            elemsWithIndices.push_back(std::make_pair(*(y+j), j));\n",
        "        std::partial_sort(elemsWithIndices.begin(), elemsWithIndices.begin() + K, elemsWithIndices.end(),\n",
        "            [] (const std::pair<float,int>& a, const std::pair<float,int>& b) { if (a.first > b.first) return true; if (a.first < b.first) return false; return a.second < b.second; });\n",
        "        for(int j = 0; j < K; ++j)\n",
        "        {\n",
        "            if ((*(z+j) != elemsWithIndices[j].second) || (*(v+j) != elemsWithIndices[j].first))\n",
        "            {\n",
        "                std::cout << getSoftmaxTopkTypeName(t) << \" mismatch for vector \" << i << \", reference (\" << elemsWithIndices[j].second << \",\" << elemsWithIndices[j].first\n",
        "                    << \"), GPU (\" << *(z+j) << \",\" << *(v+j) << \")\" << std::endl;\n",
        "                ++mismatches;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    std::cout << getSoftmaxTopkTypeName(t) << \": \" << mismatches << \" mismatches\" << std::endl;\n",
        "}\n",
        "\n",
        "void compare_softmax_topk_results(int V, int K, int batch_size, SOFTMAX_TOPK_TYPE t)\n",
        "{\n",
        "    std::vector<float> xh;\n",
        "    std::vector<int> zh;\n",
        "    std::vector<float> vh;\n",
        "\n",
        "    std::tie(xh, zh, vh) = run_topk(V, K, batch_size, t);\n",
        "\n",
        "    auto x = xh.begin();\n",
        "    auto z = zh.begin();\n",
        "    auto v = vh.begin();\n",
        "    int mismatches = 0;\n",
        "    float max_diff = 0.0F;\n",
        "    double total_diff = 0.0F;\n",
        "    for(int i = 0; i < batch_size; ++i, x += V, z += K, v += K)\n",
        "    {\n",
        "        // Compute reference softmax\n",
        "        float m = 0.0F;\n",
        "        for(int j = 0; j < V; ++j)\n",
        "            m = std::max(m, *(x+j));\n",
        "        float d = 0.0F;\n",
        "        for(int j = 0; j < V; ++j)\n",
        "            d += expf(*(x+j) - m);\n",
        "        for(int j = 0; j < V; ++j)\n",
        "            *(x+j) = expf(*(x+j) - m) / d;\n",
        "\n",
        "        std::vector<std::pair<float,int>> elemsWithIndices;\n",
        "        for(int j = 0; j < V; ++j)\n",
        "            elemsWithIndices.push_back(std::make_pair(*(x+j), j));\n",
        "        std::partial_sort(elemsWithIndices.begin(), elemsWithIndices.begin() + K, elemsWithIndices.end(),\n",
        "            [] (const std::pair<float,int>& a, const std::pair<float,int>& b) { if (a.first > b.first) return true; if (a.first < b.first) return false; return a.second < b.second; });\n",
        "        for(int j = 0; j < K; ++j)\n",
        "        {\n",
        "            float diff = fabs(*(v+j) - elemsWithIndices[j].first);\n",
        "            max_diff = std::max(max_diff, diff);\n",
        "            total_diff += diff;\n",
        "            if (*(z+j) != elemsWithIndices[j].second)\n",
        "            {\n",
        "                std::cout << getSoftmaxTopkTypeName(t) << \" mismatch for vector \" << i << \", reference (\" << elemsWithIndices[j].second << \",\" << elemsWithIndices[j].first\n",
        "                    << \"), GPU (\" << *(z+j) << \",\" << *(v+j) << \")\" << std::endl;\n",
        "                ++mismatches;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    std::cout << getSoftmaxTopkTypeName(t) << \": \" << mismatches << \" mismatches, comparing to CPU reference implementation: Max diff = \" << max_diff << \", Avg diff = \" << (float)(total_diff / (batch_size * K)) << std::endl;\n",
        "}\n",
        "\n",
        "void run_benchmark(int batch_size, int start_V, int K, int end_V, int average_run_iterations, int min_run_iteration)\n",
        "{\n",
        "    std::cout << \"Batch size = \" << batch_size << std::endl;\n",
        "    std::cout << std::setw(12) << \"V\";\n",
        "    std::cout << std::setw(20) << \"NaiveSoftmax\";\n",
        "    std::cout << std::setw(20) << \"SafeSoftmax\";\n",
        "    std::cout << std::setw(20) << \"OnlineSoftmax\";\n",
        "    std::cout << std::setw(20) << \"TopK\";\n",
        "    std::cout << std::setw(30) << \"SafeSoftmaxUnfusedTopK\";\n",
        "    std::cout << std::setw(30) << \"SafeSoftmaxFusedTopK\";\n",
        "    std::cout << std::setw(30) << \"OnlineSoftmaxFusedTopK\";\n",
        "    std::cout << std::endl;\n",
        "    float average_V = sqrtf(static_cast<float>(end_V)*static_cast<float>(start_V));\n",
        "    for(int V = start_V; V < end_V; V *= 2)\n",
        "    {\n",
        "        int run_iterations = std::max(static_cast<int>(static_cast<float>(average_run_iterations) * average_V / static_cast<float>(V)), min_run_iteration);\n",
        "        std::cout << std::setw(12) << V;\n",
        "        {\n",
        "            float runtime = benchmark_softmax(V, batch_size, SOFTMAX_TYPE_NAIVE, run_iterations);\n",
        "            std::cout << std::setw(20) << (V * batch_size / runtime);\n",
        "        }\n",
        "        {\n",
        "            float runtime = benchmark_softmax(V, batch_size, SOFTMAX_TYPE_SAFE, run_iterations);\n",
        "            std::cout << std::setw(20) << (V * batch_size / runtime);\n",
        "        }\n",
        "        {\n",
        "            float runtime = benchmark_softmax(V, batch_size, SOFTMAX_TYPE_ONLINE, run_iterations);\n",
        "            std::cout << std::setw(20) << (V * batch_size / runtime);\n",
        "        }\n",
        "        {\n",
        "            float runtime = benchmark_softmax_topk(V, K, batch_size, SOFTMAX_TOPK_TYPE_TOPK_ONLY, run_iterations);\n",
        "            std::cout << std::setw(20) << (V * batch_size / runtime);\n",
        "        }\n",
        "        {\n",
        "            float runtime = benchmark_softmax_topk(V, K, batch_size, SOFTMAX_TOPK_TYPE_SAFE_UNFUSED, run_iterations);\n",
        "            std::cout << std::setw(30) << (V * batch_size / runtime);\n",
        "        }\n",
        "        {\n",
        "            float runtime = benchmark_softmax_topk(V, K, batch_size, SOFTMAX_TOPK_TYPE_SAFE_FUSED, run_iterations);\n",
        "            std::cout << std::setw(30) << (V * batch_size / runtime);\n",
        "        }\n",
        "        {\n",
        "            float runtime = benchmark_softmax_topk(V, K, batch_size, SOFTMAX_TOPK_TYPE_ONLINE_FUSED, run_iterations);\n",
        "            std::cout << std::setw(30) << (V * batch_size / runtime);\n",
        "        }\n",
        "\n",
        "        std::cout << std::endl;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    std::cout << \"Softmax correctness check:\" << std::endl;\n",
        "    compare_softmax_results(300, 100, SOFTMAX_TYPE_NAIVE, SOFTMAX_TYPE_SAFE);\n",
        "    compare_softmax_results(300, 100, SOFTMAX_TYPE_NAIVE, SOFTMAX_TYPE_ONLINE);\n",
        "    std::cout << \"TopK correctness check:\" << std::endl;\n",
        "    compare_topk_results(300, MAX_K, 100, SOFTMAX_TOPK_TYPE_TOPK_ONLY);\n",
        "    std::cout << \"Softmax+TopK correctness check:\" << std::endl;\n",
        "    compare_softmax_topk_results(300, MAX_K, 100, SOFTMAX_TOPK_TYPE_SAFE_FUSED);\n",
        "    compare_softmax_topk_results(300, MAX_K, 100, SOFTMAX_TOPK_TYPE_ONLINE_FUSED);\n",
        "\n",
        "    int large_batch_size = 4000;\n",
        "    int small_batch_size = 10;\n",
        "    size_t max_V = 10000000;\n",
        "\n",
        "    int start_V = 63;\n",
        "    int device_id;\n",
        "    CUDA_CHECK(cudaGetDevice(&device_id));\n",
        "    cudaDeviceProp device_prop;\n",
        "    CUDA_CHECK(cudaGetDeviceProperties(&device_prop, device_id));\n",
        "    int large_batch_end_V = std::min(static_cast<size_t>(device_prop.totalGlobalMem * 0.9F) / (sizeof(float) * 3 * large_batch_size), max_V);\n",
        "    int small_batch_end_V = std::min(static_cast<size_t>(device_prop.totalGlobalMem * 0.9F) / (sizeof(float) * 3 * small_batch_size), max_V);\n",
        "\n",
        "    std::cout << \"Softmax benchmark:\" << std::endl;\n",
        "    run_benchmark(large_batch_size, start_V, MAX_K, large_batch_end_V, 100, 10);\n",
        "    run_benchmark(small_batch_size, start_V, MAX_K, small_batch_end_V, 4000, 800);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "HQo0IPGJnMYX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}